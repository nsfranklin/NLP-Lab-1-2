{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram language model lab\n",
    "\n",
    "In this lab you will do 4 exercises building and evaluating ngram language models of the following types:\n",
    "1. A Maximum Liklihood Estimation (MLE) unigram model (10 marks)\n",
    "2. A bigram model with add-one Laplace smoothing (10 marks)\n",
    "3. A bigram model with general additive (add-k) smoothing (10 marks)\n",
    "4. Ngram models with an advanced interpolation technique, Kneser-Ney snoothing (the methods are provided) (10 marks)\n",
    "\n",
    "Before you start the exercises, make sure you run and understand the examples first. Then complete the exercises using the following 3 files with line-separated text to train the bigger language models on:\n",
    "* training data -- \"switchboard_lm_training.txt\"\n",
    "* heldout/development data -- \"switchboard_lm_heldout.txt\"\n",
    "* test data -- \"switchboard_lm_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division  # for python 2 this is needed\n",
    "from __future__ import print_function # for python 2 this is needed\n",
    "from collections import Counter\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful methods for all exercises:\n",
    "def glue_tokens(tokens, order):\n",
    "    \"\"\"A useful way of glueing tokens together for\n",
    "    Kneser Ney smoothing and other smoothing methods\n",
    "    \n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    return '{0}@{1}'.format(order,' '.join(tokens))\n",
    "\n",
    "def unglue_tokens(tokenstring, order):\n",
    "    \"\"\"Ungluing tokens glued by the glue_tokens method\"\"\"\n",
    "    if order == 1:\n",
    "        return [tokenstring.split(\"@\")[1].replace(\" \",\"\")]\n",
    "    return tokenstring.split(\"@\")[1].split(\" \")\n",
    "\n",
    "def tokenize_sentence(sentence, order):\n",
    "    \"\"\"Returns a list of tokens with the correct numbers of initial\n",
    "    and end tags (this is meant ot be used with a non-backoff model!!!)\n",
    "    \n",
    "    :sentence: a string of text\n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    tokens = ['<s>'] * (order-1) + tokens + ['</s>']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of sentences (corpus) from the example in the lecture slides\n",
    "sentences = [\n",
    "            \"I am Sam\",\n",
    "            \"Sam I am\",\n",
    "            \"I do not like green eggs and ham\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Build a unigram MLE language model from a simple corpus.\n",
    "An MLE unigram model will tell you how likely a word is to occur, estimated from the function of counts:\n",
    "\n",
    "C(w_i)/N\n",
    "\n",
    "where C(w_i) is the number of times the word at position i occurred in the training corpus, and N is the sum of the counts of all words, or, to put it another way, the length of the training corpus.\n",
    "\n",
    "Notice the tokenization method adds a `</s>` at the end but no `<s>` is needed at the beginning of each sentence\n",
    "    because unigrams do not have a context word (we are only concerned with the frequency of single words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "for sent in sentences:\n",
    "    words = tokenize_sentence(sent, 1)\n",
    "    print(\"tokenized\", words)\n",
    "    for w in words:\n",
    "        unigrams[w] +=1\n",
    "unigram_total = sum(unigrams.values()) # to get the denominator for unigram probabilities\n",
    "print (len(unigrams), \"different unigrams observed\")\n",
    "print(\"unigram total\", unigram_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will measure the model's perplexity of those same training sentences. Note in normal practice you would want to do this on different data (as you will do below).\n",
    "\n",
    "Perplexity is equal to 2 to the power of the cross entropy where cross entropy is the negative sum of all log probabilities from the model normalized by the length of the corpus N.\n",
    "\n",
    "Measure the cross entropy and perplexity on each sentence too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s \n",
    "for sent in sentences:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    words = tokenize_sentence(sent, 1) # tokenize sentence with the order 1 as the parameter\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for w in words:\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        logprob = log(prob, 2)  # the log of the prob to base 2\n",
    "        s += -log(prob, 2) # add the neg log prob to s\n",
    "        sent_s += -log(prob, 2)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N\n",
    "    sent_perplexity = 2 ** sent_cross_entropy\n",
    "    print(words, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(\"unigram corpus cross entropy\", cross_entropy)\n",
    "print(\"unigram corpus perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Build a bigram MLE language model from the same corpus\n",
    "A MLE (unsmoothed) bigram model will tell you how likely a word is to occur given the previous word, estimated from the function of counts:\n",
    "\n",
    "C(w_i-1, w_i)/C(w_i-1)\n",
    "\n",
    "where for any pairs of contiguous words w_i-1, w_i, C(w_i-1, w_i) is the number of times the word at position i follows the word at position i-1 in the training corpus, and C(w_i-1) is the number of times the word at position i-1 occurs in the corpus. E.g. for the bigram probability of 'john likes', C(w_i-1, w_i) is the number of times 'john likes' occurs, and C(w_i-1) is how many times 'john' occurs.\n",
    "\n",
    "Notice the tokenization method adds a `</s>` at the end and also one `<s>` for padding at the beginning as we want to count the number of times the word at the beginning of each sentence begins a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the counts from the training corpus for bigrams without smoothing\n",
    "bigrams = Counter() # a counter for how many times a given bigram sequence w_i-1,w_i occurs\n",
    "bigram_context = Counter() # a counter for how many times each word is used as a context word w_i-1 (so will include the start symbol)\n",
    "delta = 1  # delta is order - 1\n",
    "for s in sentences:\n",
    "    words = tokenize_sentence(s, 2)  # tokenize sentence with the order 2 as the parameter\n",
    "    for i in range(delta, len(words)):\n",
    "        context = words[i-delta:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        bigrams[glue_tokens(ngram, 2)] +=1\n",
    "        bigram_context[glue_tokens(context, 1)] += 1\n",
    "print(len(bigrams.keys()), \"different bigrams\")\n",
    "print(len(bigram_context.keys()), \"different bigram contexts (and unigrams) observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a handy function to calculate the probability of an bigram from the counts\n",
    "def prob_bigram_MLE(ngram):\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = numerator / denominator\n",
    "    return prob\n",
    "\n",
    "\n",
    "# check if each bigram continuation distribution sums to one\n",
    "# look at the distributions of possible continuations after each word\n",
    "for context, v in bigram_context.items():\n",
    "    context = unglue_tokens(context, 1)\n",
    "    print(\"%% context\", context)\n",
    "    check_ngram_total_sums_to_1 = 0\n",
    "    # for a given context the continuation probabilities \n",
    "    # over the whole vocab should sum to 1\n",
    "    for u in unigrams.keys():\n",
    "        ngram = context + [u]\n",
    "        prob = prob_bigram_MLE(ngram)\n",
    "        print(ngram, prob)\n",
    "        check_ngram_total_sums_to_1 += prob\n",
    "    print(\"sums to 1?\", check_ngram_total_sums_to_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.5\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Check the estimates for the lecture examples:\n",
    "# p(I|<s>)\n",
    "# p(Sam|<s>)\n",
    "# p(am|I)\n",
    "# p(</s>|Sam)\n",
    "# p(Sam|am)\n",
    "# p(do|I)\n",
    "\n",
    "print(prob_bigram_MLE(['<s>','I']))\n",
    "print(prob_bigram_MLE(['<s>', 'Sam']))\n",
    "print(prob_bigram_MLE(['I', 'am']))\n",
    "print(prob_bigram_MLE(['Sam', '</s>']))\n",
    "print(prob_bigram_MLE(['am', 'Sam']))\n",
    "print(prob_bigram_MLE(['I', 'do']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, as in the unigram case above we will measure the model's perplexity of those same training sentences.\n",
    "\n",
    "Notice that even with this small corpus the bigram perplexity is significantly lower than the unigram perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s\n",
    "for sent in sentences:\n",
    "    words = tokenize_sentence(sent, 2)\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for i in range(delta, len(words)):\n",
    "        context = words[i-delta:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        prob = prob_bigram_MLE(ngram)\n",
    "        s += -log(prob, 2) # add the neg log prob to s\n",
    "        sent_s += -log(prob, 2)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N\n",
    "    sent_perplexity = 2 ** sent_cross_entropy\n",
    "    print(words, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(\"bigram corpus cross entropy\", cross_entropy)\n",
    "print(\"bigram corpus perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Unigram MLE model from a bigger corpus\n",
    "\n",
    "Write code to read in the file `switchboard_language_model_train.txt` which has preprocessed text on each line.\n",
    "\n",
    "You will populate a unigram language model based on that data for an MLE estimation using a `Counter` using this data (see Example 1 above as to how this is done for a smaller dataset).\n",
    "\n",
    "Before you do this, you have to define a vocabulary of words using the training data, which you will keep the same for all of the following exercises. In these exercises, the vocabulary is defined by using a Minimum Document Frequency of 2 in the training data. That means the vocab should only contain words which occur at least twice in the training data.\n",
    "\n",
    "Any words not in the vocabulary in the training, heldout and testing data must be replaced with an out-of-vocab symbol `<unk/>` before processing them.\n",
    " \n",
    "Using this model, calculate the perplexity of the ENTIRE test corpus `switchboard_language_model_test.txt`- again, remember to replace words unknown by the model with `<unk/>` before getting these measures. See Example 1 as to how this is done for a unigram model on the smaller example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 2. Bigram model with add-one smoothing\n",
    "\n",
    "Change your method for reading in and training a language model from Exercise 1 so it works for bigrams. Use the same methods for identifying and replacing out-of-vocabulary words as you did in Exercise 1.\n",
    "\n",
    "However, in testing, using these raw counts rather than simply implementing MLE you should implement add-one smoothing (see the lecture notes and Jurafsky & Martin Chapter 3/Manning and Schuetze Chapter 6). Remember this involves using the vocabulary size.\n",
    "\n",
    "Obtain the perplexity score on the test data as above for this smoothed bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3. Bigram model with general additive (Lidstone) add-k smoothing\n",
    "\n",
    "Modify your code from Exercise 2 such that it generalizes beyond adding 1 to all counts, but can add differing amounts k of mass instead, to implement general additive add-k smoothing.\n",
    "\n",
    "On the HELDOUT corpus `switchboard_language_model_heldout.txt` experiment with different values of k (e.g. 0.2, 0.4, 0.6, 0.8, though try others if you can) and report the perplexity scores for these different values in a comment.\n",
    "\n",
    "Once you find the value which gives you the lowest perplexity on the heldout data, use this model to get the perplexity of the test data once and report the scores in a comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 4. Ngram models with Kneser-Ney smoothing\n",
    "\n",
    "Kneser-Ney smoothing is a state-of-the-art technique for smoothing n-gram models.\n",
    "\n",
    "The algorithm is quite complicated, and is implemented for you below for training/getting the appropriate counts  on the training data (`ngrams_interpolated_kneser_ney()`). The training process is implemented below for a trigram model, but without doing the appropriate out-of-vocab word replacement as you've done above.\n",
    "\n",
    "The application at test time is done with the method `kneser_ney_ngram_prob()` using the trained Counters, which gives the probability of the model applied to an ngram of a given order, with a given discount.\n",
    "\n",
    "Try to follow how the training works and how the application of the model to ngrams works, and refer to the below article on QM plus (pages 7-8 particularly):\n",
    "\n",
    "\"A Bit of Progress in Language Modeling\" (2001) - Joshua T. Goodman\n",
    "\n",
    "In this exercise, you will first modify the training part of the code so it does the replacement of out-of-vocab words as you did in the previous exercises. You do not need to modify the methods below.\n",
    "\n",
    "On the HELDOUT corpus experiment with different orders from trigram upwards (try 3, 4 and 5) and different discount values (e.g. 0.2, 0.4, 0.6, 0.8, though try others if you can) and report the perplexity scores for these different values in a comment. \n",
    "\n",
    "Once you find the order and discount values which gives you the lowest perplexity on the heldout data, use this model to get the perplexity of the TEST data once and report the scores in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneser-Ney smoothing\n",
    "order = 3\n",
    "discount = 0.8\n",
    "\n",
    "unigram_denominator = 0\n",
    "ngram_numerator_map = Counter() \n",
    "ngram_denominator_map = Counter() \n",
    "ngram_non_zero_map = Counter()\n",
    "\n",
    "\n",
    "def ngrams_interpolated_kneser_ney(tokens,\n",
    "                                   order,\n",
    "                                   ngram_numerator_map,\n",
    "                                   ngram_denominator_map,\n",
    "                                   ngram_non_zero_map,\n",
    "                                   unigram_denominator):\n",
    "    \"\"\"Function used in n-gram language model training\n",
    "    to count the n-grams in tokens and also record the\n",
    "    lower order non -ero counts necessary for interpolated Kneser-Ney\n",
    "    smoothing.\n",
    "    \n",
    "    Taken from Goodman 2001 and generalized to arbitrary orders\"\"\"\n",
    "    for i in xrange(order-1,len(tokens)): # tokens should have a prefix of order - 1\n",
    "        #print i\n",
    "        for d in xrange(order,0,-1): #go through all the different 'n's\n",
    "            if d == 1:\n",
    "                unigram_denominator += 1\n",
    "                ngram_numerator_map[glue_tokens(tokens[i],d)] += 1\n",
    "            else:\n",
    "                den_key = glue_tokens(tokens[i-(d-1) : i], d)\n",
    "                num_key = glue_tokens(tokens[i-(d-1) : i+1], d)\n",
    "    \n",
    "                ngram_denominator_map[den_key] += 1\n",
    "                # we store this value to check if it's 0\n",
    "                tmp = ngram_numerator_map[num_key]\n",
    "                ngram_numerator_map[num_key] += 1 # we increment it\n",
    "                if tmp == 0: # if this is the first time we see this ngram\n",
    "                    #number of types it's been used as a context for\n",
    "                    ngram_non_zero_map[den_key] += 1\n",
    "                else:\n",
    "                    break \n",
    "                    # if the ngram has already been seen\n",
    "                    # we don't go down to lower order models\n",
    "    return ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "corpus = open(\"switchboard_lm_train.txt\")\n",
    "for line in corpus:\n",
    "    tokens = tokenize_sentence(line, order)\n",
    "    ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator =\\\n",
    "            ngrams_interpolated_kneser_ney(tokens,\n",
    "                                           order,\n",
    "                                           ngram_numerator_map,\n",
    "                                           ngram_denominator_map,\n",
    "                                           ngram_non_zero_map,\n",
    "                                           unigram_denominator)\n",
    "corpus.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kneser_ney_ngram_prob(ngram, discount, order):\n",
    "    \"\"\"KN smoothed ngram probability from Goodman 2001.\n",
    "    This is run at test time to calculate the probability\n",
    "    of a given n-gram or a given order with a given discount.\n",
    "    \n",
    "    ngram :: list of strings, the ngram\n",
    "    discount :: float, the discount used (lambda)\n",
    "    order :: int, order of the model\n",
    "    \"\"\"\n",
    "    # First, calculate the unigram prob of the last token \n",
    "    # If we've never seen it at all, it will \n",
    "    # have no probability as a numerator\n",
    "    uni_num = ngram_numerator_map.get(glue_tokens(ngram[-1], 1))\n",
    "    if not uni_num: # if no value found in dict, make it 0\n",
    "        uni_num = 0\n",
    "    probability = previous_prob = float(uni_num) / float(unigram_denominator)\n",
    "    \n",
    "    # Given <unk/> should have been used in place of unknown words before passing\n",
    "    # to this method,\n",
    "    # probability should be non-zero\n",
    "    if probability == 0.0:\n",
    "        print(\"0 prob for unigram!\")\n",
    "        print(glue_tokens(ngram[-1], 1))\n",
    "        print(ngram)\n",
    "        print(ngram_numerator_map.get(glue_tokens(ngram[-1], 1)))\n",
    "        print(unigram_denominator)\n",
    "        raise Exception\n",
    "\n",
    "    # Compute the higher order probs (from 2/bi-gram upwards) and interpolate them\n",
    "    for d in xrange(2,order+1):\n",
    "        # Get the number of times this denominator has been seen as one\n",
    "        # For bigrams this is the number of different continuation types counted\n",
    "        ngram_den = ngram_denominator_map.get(glue_tokens(ngram[-(d):-1], d))\n",
    "        if not ngram_den: # if no value found in dict, make it 0\n",
    "            ngram_den = 0\n",
    "        if ngram_den != 0: \n",
    "            ngram_num = ngram_numerator_map.get(glue_tokens(ngram[-(d):], d))\n",
    "            if not ngram_num: # if no value found in dict, make it 0\n",
    "                ngram_num = 0\n",
    "            if ngram_num != 0:\n",
    "                current_prob = (ngram_num - discount) / float(ngram_den)\n",
    "            else:\n",
    "                current_prob = 0.0\n",
    "            nonzero = ngram_non_zero_map.get(glue_tokens(ngram[-(d):-1], d))\n",
    "            if not nonzero: # if no value found in dict, make it 0\n",
    "                nonzero = 0\n",
    "            # interpolate with previous probability of lower orders calculated\n",
    "            # so far\n",
    "            current_prob += nonzero * discount / ngram_den * previous_prob\n",
    "            previous_prob = current_prob\n",
    "            probability = current_prob\n",
    "        else:\n",
    "            #if this context (e.g. bigram contect for trigrams) has never been seen, \n",
    "            #then we can only get the last order with a probability (e.g. unigram)\n",
    "            #and halt\n",
    "            probability = previous_prob\n",
    "            break\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
