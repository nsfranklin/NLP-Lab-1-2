{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as Skmet #used for precision_recall_fscore_support()\n",
    "from operator import itemgetter #used to unpack turples\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        count = 0\n",
    "        for line in reader:\n",
    "            if count == 0:  #edit to remove the headers from the text data.\n",
    "                count += 1\n",
    "            else:\n",
    "                (Id, Text, Label) = parseReview(line)\n",
    "                rawData.append((Id, Text, Label))\n",
    "                preprocessedData.append((Id, preProcess(Text), Label))\n",
    "                count += 1\n",
    "        \n",
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    #print(reviewLine)\n",
    "    Id = reviewLine[0]\n",
    "    Text = reviewLine[8]\n",
    "    if reviewLine[1] == '__label2__':\n",
    "        Label = 'real'\n",
    "    else:\n",
    "        Label = 'fake'\n",
    "    return (Id, Text, Label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "def padWithBlank(found):\n",
    "    if found == \"%\":\n",
    "        return \" % \"\n",
    "    elif found == \"!\":\n",
    "        return \" ! \"\n",
    "    elif found == \"?\":\n",
    "        return \" ? \"\n",
    "    \n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    #I noticed html tags in the text. Often joining words without a space\n",
    "    #so all <*> occurances are replaced with \" \"\n",
    "    text = re.sub(r\"<.*>\", \" \", text) # html tag removal\n",
    "    \n",
    "    \n",
    "    #Set a minium document frequency 2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    temp = text.split(\" \")\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2\n",
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    reviewDict = { }\n",
    "    for token in tokens: #adding to the review dict\n",
    "        if token in reviewDict:\n",
    "            reviewDict[token] = reviewDict[token] + 1\n",
    "        else:\n",
    "            reviewDict[token] = 1\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in featureDict:\n",
    "            featureDict[token] = featureDict[token] + 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "    return reviewDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 3\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    cv_results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        \n",
    "        #training\n",
    "        classifier = trainClassifier(tempDataSet)\n",
    "        \n",
    "        #classifing\n",
    "        results = predictLabels(testingSet, classifier)\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,pos_label=\"fake\")\n",
    "        recall = Skmet.recall_score(trueLabels, results,pos_label=\"fake\")\n",
    "        f_score = 2 * (precision * recall)/(precision + recall)\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results)\n",
    "        cv_results.append((precision, recall, f_score, accuracy)) #create a turple for metrics for each fold\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to c:/nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "71941\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Precision, Recall, Fscore, Accuracy\n",
      "(0.49166666666666664, 1.0, 0.659217877094972, 0.49166666666666664)\n",
      "(0.4928571428571429, 1.0, 0.6602870813397129, 0.4928571428571429)\n",
      "(0.5023809523809524, 1.0, 0.6687797147385103, 0.5023809523809524)\n",
      "(0.0, 0.0, nan, 0.0)\n",
      "(0.5261904761904762, 1.0, 0.6895475819032763, 0.5261904761904762)\n",
      "(0.4886904761904762, 1.0, 0.6565373850459816, 0.4886904761904762)\n",
      "(0.0, 0.0, nan, 0.0)\n",
      "(0.5196428571428572, 1.0, 0.6839012925969448, 0.5196428571428572)\n",
      "(0.4988095238095238, 1.0, 0.6656076250992852, 0.4988095238095238)\n",
      "(0.0, 0.0, nan, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "#download lemminzer data\n",
    "nltk.download(\"wordnet\", \"c:/nltk_data/\")\n",
    "\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "crossValidationResults = crossValidate(trainData, 10);\n",
    "\n",
    "print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "for i in range(0,len(crossValidationResults)):\n",
    "    print(crossValidationResults[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Pre Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.0, 0.0, 0.0, 0.4875) #This result gave a convergence warning so precision, recall and fscore set to 0.\n",
    "(0.4886904761904762, 1.0, 0.6565373850459816, 0.4886904761904762)\n",
    "(1.0, 1.0, 1.0, 1.0)\n",
    "(0.5029761904761905, 1.0, 0.6693069306930693, 0.5029761904761905)\n",
    "(1.0, 1.0, 1.0, 1.0)\n",
    "(0.48333333333333334, 1.0, 0.6516853932584269, 0.48333333333333334)\n",
    "(0.49047619047619045, 1.0, 0.65814696485623, 0.49047619047619045)\n",
    "(1.0, 1.0, 1.0, 1.0)\n",
    "(0.5023809523809524, 1.0, 0.6687797147385103, 0.5023809523809524)\n",
    "(0.5125, 1.0, 0.6776859504132231, 0.5125)\n",
    "\n",
    "Not great. The accuracy is very bad you would get this from random guessing the class. Slightly better than guessin the same for everything but still poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve Preprocessing\n",
    "    \n",
    "    ####removed html tags (mainly <br/>) \n",
    "    \n",
    "    The addition of tag removal reduced the feature count by 17815 from 89756 to 71941\n",
    "(1.0, 1.0, 1.0, 1.0)\n",
    "(0.0, 0.0, 0.0, 0.0)\n",
    "(0.48273809523809524, 1.0, 0.65114411882778, 0.48273809523809524)\n",
    "(0.0, 0.0, 0.0, 0.0)\n",
    "(1.0, 1.0, 1.0, 1.0)\n",
    "(0.0, 0.0, 0.0, 0.0)\n",
    "(0.5029761904761905, 1.0, 0.6693069306930693, 0.5029761904761905)\n",
    "(0.5029761904761905, 1.0, 0.6693069306930693, 0.5029761904761905)\n",
    "(0.0, 0.0, 0.0, 0.0)\n",
    "(0.49642857142857144, 1.0, 0.6634844868735084, 0.49642857142857144)\n",
    "    \n",
    "    lemmatisation\n",
    "\n",
    "Normalising Punctuation\n",
    "    \n",
    "    \n",
    "    \n",
    "Alternatives to Unigrams\n",
    "\n",
    "    Results\n",
    "    \n",
    "Change Class Weights\n",
    "\n",
    "    Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the review from a varified purchaser\n",
    "\n",
    "    Results\n",
    "    \n",
    "Length of the review text\n",
    "\n",
    "    Results\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
