{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as Skmet #used for precision_recall_fscore_support()\n",
    "from operator import itemgetter #used to unpack turples\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        count = 0\n",
    "        for line in reader:\n",
    "            if count == 0:  #edit to remove the headers from the text data.\n",
    "                count += 1\n",
    "            else:\n",
    "                (Id, Text, Label, Verified, Title) = parseReview(line)\n",
    "                rawData.append((Id, Text, Label, Verified, Title))\n",
    "                preprocessedData.append((Id, preProcess(Text), Label, Verified, preProcess(Title)))\n",
    "                count += 1\n",
    "        \n",
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label, Verified, Title) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text),preProcess(Title),Verified),Label))\n",
    "    for (_, Text, Label, Verified, Title) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text),preProcess(Title),Verified),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    #print(reviewLine)\n",
    "    Id = reviewLine[0] #Assigns the ID, review text and alters the label. #ex5 extension: the edition of Verified.\n",
    "    Text = reviewLine[8] #\n",
    "    Verified = reviewLine[3] \n",
    "    if reviewLine[1] == '__label2__':\n",
    "        Label = 'real'\n",
    "    else:\n",
    "        Label = 'fake'\n",
    "    Title = reviewLine[7]\n",
    "    \n",
    "    return (Id, Text, Label, Verified, Title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    #I noticed html tags in the text. Often joining words without a space\n",
    "    #so all <*> occurances are replaced with \" \"\n",
    "    text = text.lower() #normalize the text \n",
    "    text = re.sub(r\"<.*>\", \" \", text) # html tag removal\n",
    "    text = re.sub(r\".\\'\", \"\" , text) # to prevent 'don't' becoming 'don' 't'\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    preNormTemp = tokenizer.tokenize(text) #applying regex tokenizer to remove punctuation.\n",
    "\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "     \n",
    "    #temp = []\n",
    "    #stopWords = set(stopwords.words('english')) #stop word removal.\n",
    "    #for w in preNormTemp:\n",
    "    #    if (w not in stopWords):\n",
    "    #        temp.append(w)#lemmatizer.lemmatize(w))\n",
    "    #temp = text.split(\" \")\n",
    "        \n",
    "    return preNormTemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2\n",
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens,titles,verified):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    reviewDict = {}\n",
    "    for token in tokens: #adding to the review dict\n",
    "        if token in reviewDict:\n",
    "            reviewDict[token] = reviewDict[token] + 1\n",
    "        else:\n",
    "            reviewDict[token] = 1\n",
    "    \n",
    "    for title in titles:\n",
    "        if title in reviewDict:\n",
    "            reviewDict[title] = reviewDict[title] + 1\n",
    "        else:\n",
    "            reviewDict[title] = 1\n",
    "    \n",
    "    if verified == \"Y\":\n",
    "        reviewDict[\"VerifiedToken\"] = 1\n",
    "    \n",
    "    reviewDict[\"reviewLength\"] = len(tokens) # ex5 adds the review length feature with weight = to number of tokens in review\n",
    "    \n",
    "    for token in tokens: #adds values to featureDict\n",
    "        if token in featureDict:\n",
    "            featureDict[token] = featureDict[token] + 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "    \n",
    "    return reviewDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 3\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #print(testingSetRemovedLabel)\n",
    "        #training\n",
    "        classifier = trainClassifier(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabel(i,classifier))\n",
    "        \n",
    "        #predictLabels using 1/10th of the dataset\n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,pos_label=\"fake\") #finds precision score\n",
    "        recall = Skmet.recall_score(trueLabels, results,pos_label=\"fake\") #finds recall score\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify((reviewSample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nickf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nickf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "31528\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickf\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Precision, Recall, Fscore, Accuracy\n",
      "(0.6123057913391206, 0.6182430884511234, 0.600991956523832, 0.607202380952381)\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "#download nltk data\n",
    "nltk.download(\"wordnet\") #word set for lemmatizing\n",
    "nltk.download(\"stopwords\") #stop word dataset for stopword removal\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "crossValidationResults = crossValidate(trainData, 10);\n",
    "\n",
    "print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "print(crossValidationResults) # prints cross validation results.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Pre Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.6436093620282846, 0.6562403435435953, 0.6497714381913016, 0.6463690476190477)\n",
    "\n",
    "Not great. The accuracy is bad but better than randomly guessing the class and a better f_score than classifying the whole set as the same class for everything but still very poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve Preprocessing\n",
    "      \n",
    "        I noticed it might be a good idea to make everything lowercase. Reduced the feature count from 98689 to 78556. This didn't increase the effectiveness of the classification.\n",
    "        \n",
    "        Precision, Recall, Fscore, Accuracy\n",
    "        (0.6354279957939459, 0.6546865208273466, 0.6446480291722718, 0.6392857142857142)\n",
    "\n",
    "Remove html tags\n",
    "\n",
    "        I removed html tags mostly break tags. The addition of tag removal reduced the feature count by 17815 from 78556 to 63038. Didn't improve the poor performance. \n",
    "   \n",
    "        Precision, Recall, Fscore, Accuracy\n",
    "        (0.6295785761348717, 0.6499272215428664, 0.6395140434003548, 0.633690476190476)\n",
    " \n",
    "Punctuation Removal\n",
    "\n",
    "        Next I removed all punctuation. First removing ' without replacing with a space to avoid \"don't\" to \"don\" \"t\". This reduced the features to 31528. The accuracy decreased further. I should probably maintain some punctuation.\n",
    "      \n",
    "        Precision, Recall, Fscore, Accuracy\n",
    "        (0.6010034972565854, 0.6248868779572152, 0.612596200212048, 0.6049404761904762)      \n",
    "\n",
    "Stop word removal\n",
    "\n",
    "        Stop word removal reduce the feature count from 31528 to 31384. But none of the scores increased.\n",
    "   \n",
    "        Precision, Recall, Fscore, Accuracy\n",
    "        (0.5941561103254681, 0.6171329835048187, 0.6052725735879014, 0.5976190476190477)\n",
    " \n",
    "Lemmatization\n",
    "   \n",
    "       I lemmatized the data this reduced the feature count further from 31528 to 27759. There was a slight improvement over the previous version without lemmatization. \n",
    "   \n",
    "        Precision, Recall, Fscore, Accuracy\n",
    "        (0.5999308331787133, 0.6240195903442175, 0.6113552589768079, 0.6036904761904761)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of the review text\n",
    "\n",
    "    I was curious to see if the lenght of the text was a useful feature to consider. I created a feature key reviewLength that weight was the length of the review. There was an improvement however repeated tested didn't always result in the same values.\n",
    "        Precision, Recall, Fscore, Accuracy\n",
    "        (0.6123057913391206, 0.6182430884511234, 0.600991956523832, 0.607202380952381)\n",
    "    \n",
    "\n",
    "Is the review from a varified purchaser and the inclusion of a title feature.\n",
    "\n",
    "    I modified crossvalidation to use predicate label so that I was able to pass in more features. These features were the title of the review and if the review was varified. I gave each word in the title a weight of 3 making the assumption that the title was likely to distill the point of the review because it is there to get readers to read the whole review therefore is representative of the review. I also gave the a weight of 5 if the review was varified. The results got much better after adding these features seeing a 13% improvement.\n",
    "    \n",
    "    Precision, Recall, Fscore, Accuracy\n",
    "    (0.7302124656407247, 0.7394123539961068, 0.7346843002043812, 0.7333333333333333)\n",
    "\n",
    "    I altered the weights to see if the accuracy returned. Verified 2 and 2 for words in the title. Improved it only very slightly.\n",
    "    \n",
    "    Precision, Recall, Fscore, Accuracy\n",
    "    (0.7373699041694369, 0.7426404459824403, 0.7398504102592587, 0.7391666666666666)\n",
    "    \n",
    "    I removed the feature of the length of the text. As on inspection this was always the largest weight by a large amount.\n",
    "  \n",
    "    Precision, Recall, Fscore, Accuracy \n",
    "    (0.7335934192445652, 0.738770453224032, 0.7359985523759297, 0.7352976190476191)\n",
    "    \n",
    "    Further reduced the title weight and Verified value. 1 for the weight and if it was verified.\n",
    "\n",
    "    Precision, Recall, Fscore, Accuracy Further Minior Improvements.\n",
    "    (0.7416190589082572, 0.7522645420762297, 0.7467738416749492, 0.7451190476190476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
