{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_tokens(tokens, order):\n",
    "    \"\"\"A useful way of glueing tokens together for\n",
    "    Kneser Ney smoothing and other smoothing methods\n",
    "    \n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    return '{0}@{1}'.format(order,' '.join(tokens))\n",
    "\n",
    "def unglue_tokens(tokenstring, order):\n",
    "    \"\"\"Ungluing tokens glued by the glue_tokens method\"\"\"\n",
    "    if order == 1:\n",
    "        return [tokenstring.split(\"@\")[1].replace(\" \",\"\")]\n",
    "    return tokenstring.split(\"@\")[1].split(\" \")\n",
    "\n",
    "def tokenize_sentence(sentence, order):\n",
    "    \"\"\"Returns a list of tokens with the correct numbers of initial\n",
    "    and end tags (this is meant ot be used with a non-backoff model!!!)\n",
    "    \n",
    "    :sentence: a string of text\n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    tokens = ['<s>'] * (order-1) + tokens + ['</s>']\n",
    "    return tokens\n",
    "\n",
    "def find_unigrams(filename, MDF): # ex 1 ; #finds the unigram with a set minium document frequency. Key values used as a vocab\n",
    "    dataSource = open(filename, 'r')\n",
    "    unigrams = Counter()\n",
    "\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,1)\n",
    "        for w in words:\n",
    "            unigrams[w] += 1\n",
    "\n",
    "    #eremoves words under the MDF.\n",
    "    unigramToRemove = [k for k in unigrams if unigrams[k] < MDF] #Combind training data pass 1 and 2 into one pass.\n",
    "    for w in unigramToRemove:\n",
    "        unigrams[\"<unk/>\"] += unigrams[w] #allow for variable MDF\n",
    "        del unigrams[w]\n",
    "\n",
    "    return unigrams\n",
    "\n",
    "def readInAndUnkOOV(filename, vocabCount): # exercise 1\n",
    "    dataSource = open(filename, 'r')\n",
    "    dataSansOOV = [];\n",
    "    for line in dataSource: #reads in a files and UNKifies words that are not in the vocab.\n",
    "        words = tokenize_sentence(line,1)\n",
    "        words[:] = ['<unk/>' if not(x in vocabCount) else x for x in words]\n",
    "        dataSansOOV.append(words)\n",
    "    \n",
    "    return dataSansOOV\n",
    "        \n",
    "def readInUnkOOVRejoin(filename, vocabCount): #a readIn Varient. UNKifies data and joins it back into an list of lists.\n",
    "    dataSansOOV = readInAndUnkOOV(filename, vocabCount)\n",
    "    joinedData = [];\n",
    "    for x in dataSansOOV:\n",
    "        joinedData.append(' '.join(x))\n",
    "    return joinedData\n",
    "\n",
    "\n",
    "#calculates the bigrams in a file with MDF defined by the vocab.    \n",
    "def find_bigrams(filename, vocab, MDF): # exercise 2. Vocab represented by Unigram\n",
    "    dataSource = open(filename, 'r')\n",
    "    dataSansOOV = [];\n",
    "    for line in dataSource:\n",
    "        words = line.split(\" \")\n",
    "        words[:] = ['<unk/>' if not(x in vocab) else x for x in words]\n",
    "        dataSansOOV.append(' '.join(words)) #rejoins the words now Unkified        \n",
    "    \n",
    "\n",
    "    bigrams = Counter() # a counter for how many times a given bigram sequence w_i-1,w_i occurs\n",
    "    bigram_context = Counter() # a counter for how many times each word is used as a context word w_i-1 (so will include the start symbol)\n",
    "    delta = 1  # delta is order - 1\n",
    "    for line in dataSansOOV:\n",
    "        words = tokenize_sentence(line, 2)  # tokenize sentence with the order 2 as the parameter\n",
    "        for i in range(delta, len(words)):\n",
    "            context = words[i-delta:i]\n",
    "            target = words[i]\n",
    "            ngram = context + [target]\n",
    "            bigrams[glue_tokens(ngram, 2)] +=1\n",
    "            bigram_context[glue_tokens(context, 1)] += 1\n",
    "    print(len(bigrams.keys()), \"different bigrams\")\n",
    "    print(len(bigram_context.keys()), \"different bigram contexts (and unigrams) observed\")\n",
    "\n",
    "    return bigrams, bigram_context\n",
    "        \n",
    "def prob_bigram_add_one(ngram, i): # computes add one probability of an input ngram\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = (numerator + 1) / (denominator + i)\n",
    "    return prob\n",
    "\n",
    "def prob_bigram_add_k(ngram, i, k): #computes the add k probability of an input ngram\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = (numerator + k) / (denominator + k*i)\n",
    "    return prob\n",
    "\n",
    "def prob_bigram_MLE(ngram): #example method\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = numerator / denominator\n",
    "    return prob\n",
    "\n",
    "def readIn(filename, mode): # execise 2 mostly Reads in a file and splits it.\n",
    "    dataSource = open(filename, 'r')\n",
    "    data = [];\n",
    "    if mode == 1:\n",
    "        for line in dataSource:\n",
    "            words = tokenize_sentence(line,1)\n",
    "            data.append(words)\n",
    "    else:\n",
    "        count = 0\n",
    "        for line in dataSource:\n",
    "            words = tokenize_sentence(line,1)\n",
    "            data.append(words)\n",
    "            count += 1\n",
    "            if count > 1000:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def kneser_ney_ngram_prob(ngram, discount, order): \n",
    "    \"\"\"KN smoothed ngram probability from Goodman 2001.\n",
    "    This is run at test time to calculate the probability\n",
    "    of a given n-gram or a given order with a given discount.\n",
    "    \n",
    "    ngram :: list of strings, the ngram\n",
    "    discount :: float, the discount used (lambda)\n",
    "    order :: int, order of the model\n",
    "    \"\"\"\n",
    "    # First, calculate the unigram prob of the last token \n",
    "    # If we've never seen it at all, it will \n",
    "    # have no probability as a numerator\n",
    "    uni_num = ngram_numerator_map.get(glue_tokens(ngram[-1], 1))\n",
    "    if not uni_num: # if no value found in dict, make it 0\n",
    "        uni_num = 0\n",
    "    probability = previous_prob = float(uni_num) / float(unigram_denominator)\n",
    "    \n",
    "    # Given <unk/> should have been used in place of unknown words before passing\n",
    "    # to this method,\n",
    "    # probability should be non-zero\n",
    "    if probability == 0.0:\n",
    "        print(\"0 prob for unigram!\")\n",
    "        print(glue_tokens(ngram[-1], 1))\n",
    "        print(ngram)\n",
    "        print(ngram_numerator_map.get(glue_tokens(ngram[-1], 1)))\n",
    "        print(unigram_denominator)\n",
    "        raise Exception\n",
    "\n",
    "    # Compute the higher order probs (from 2/bi-gram upwards) and interpolate them\n",
    "    for d in range(2,order+1):\n",
    "        # Get the number of times this denominator has been seen as one\n",
    "        # For bigrams this is the number of different continuation types counted\n",
    "        ngram_den = ngram_denominator_map.get(glue_tokens(ngram[-(d):-1], d))\n",
    "        if not ngram_den: # if no value found in dict, make it 0\n",
    "            ngram_den = 0\n",
    "        if ngram_den != 0: \n",
    "            ngram_num = ngram_numerator_map.get(glue_tokens(ngram[-(d):], d))\n",
    "            if not ngram_num: # if no value found in dict, make it 0\n",
    "                ngram_num = 0\n",
    "            if ngram_num != 0:\n",
    "                current_prob = (ngram_num - discount) / float(ngram_den)\n",
    "            else:\n",
    "                current_prob = 0.0\n",
    "            nonzero = ngram_non_zero_map.get(glue_tokens(ngram[-(d):-1], d))\n",
    "            if not nonzero: # if no value found in dict, make it 0\n",
    "                nonzero = 0\n",
    "            # interpolate with previous probability of lower orders calculated\n",
    "            # so far\n",
    "            current_prob += nonzero * discount / ngram_den * previous_prob\n",
    "            previous_prob = current_prob\n",
    "            probability = current_prob\n",
    "        else:\n",
    "            #if this context (e.g. bigram contect for trigrams) has never been seen, \n",
    "            #then we can only get the last order with a probability (e.g. unigram)\n",
    "            #and halt\n",
    "            probability = previous_prob\n",
    "            break\n",
    "    return probability\n",
    "\n",
    "def ngrams_interpolated_kneser_ney(tokens,\n",
    "                                   order,\n",
    "                                   ngram_numerator_map,\n",
    "                                   ngram_denominator_map,\n",
    "                                   ngram_non_zero_map,\n",
    "                                   unigram_denominator):\n",
    "    \"\"\"Function used in n-gram language model training\n",
    "    to count the n-grams in tokens and also record the\n",
    "    lower order non -ero counts necessary for interpolated Kneser-Ney\n",
    "    smoothing.\n",
    "    \n",
    "    Taken from Goodman 2001 and generalized to arbitrary orders\"\"\"\n",
    "    for i in range(order-1,len(tokens)): # tokens should have a prefix of order - 1\n",
    "        #print i\n",
    "        for d in range(order,0,-1): #go through all the different 'n's\n",
    "            if d == 1:\n",
    "                unigram_denominator += 1\n",
    "                ngram_numerator_map[glue_tokens(tokens[i],d)] += 1\n",
    "            else:\n",
    "                den_key = glue_tokens(tokens[i-(d-1) : i], d)\n",
    "                num_key = glue_tokens(tokens[i-(d-1) : i+1], d)\n",
    "    \n",
    "                ngram_denominator_map[den_key] += 1\n",
    "                # we store this value to check if it's 0\n",
    "                tmp = ngram_numerator_map[num_key]\n",
    "                ngram_numerator_map[num_key] += 1 # we increment it\n",
    "                if tmp == 0: # if this is the first time we see this ngram\n",
    "                    #number of types it's been used as a context for\n",
    "                    ngram_non_zero_map[den_key] += 1\n",
    "                else:\n",
    "                    break \n",
    "                    # if the ngram has already been seen\n",
    "                    # we don't go down to lower order models\n",
    "    return ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12730 Unique unigrams observed\n",
      "unigram total 1306313\n",
      "Test corpus cross entropy 8.287702715147693\n",
      "Test corpus perplexity 312.4979066155026\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,2) #the key values of the unigram are also a vocab.\n",
    "\n",
    "unigram_total = sum(unigrams.values());\n",
    "print(len(unigrams), \"Unique unigrams observed\")\n",
    "print(\"unigram total\", unigram_total)\n",
    "\n",
    "testData = readInAndUnkOOV(fileTesting, unigrams) # Reads int he testdata and UNKifies OOV words.\n",
    "\n",
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s \n",
    "for sent in testData:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for w in sent:\n",
    "        #print(str(unigrams[w]) + \" \" + w)\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        s += -log(prob, 2) # add the neg log prob to s\n",
    "        sent_s += -log(prob, 2)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N\n",
    "    sent_perplexity = 2 ** sent_cross_entropy\n",
    "    #print(sent, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(\"Test corpus cross entropy\", cross_entropy)\n",
    "print(\"Test corpus perplexity\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183922 different bigrams\n",
      "12225 different bigram contexts (and unigrams) observed\n",
      "7898\n",
      "10.061359048452715 Cross Entropy\n",
      "1068.4910057009279 Perplexity\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,2) #used to contruct MDF 2 Vocab from training set.\n",
    "bigrams, bigram_context = find_bigrams(fileTraining, unigrams,2) #trains a bigram\n",
    "\n",
    "#print(unigrams[\"<unk/>\"])\n",
    "#add one smoothing implementation\n",
    "\n",
    "testData = readIn(fileTesting, 1) #reads in test data\n",
    "\n",
    "s = 0\n",
    "N = 0\n",
    "delta = 1\n",
    "\n",
    "for sent in testData: # standard calculation of the cross entropy\n",
    "    for i in range(delta, len(sent)):\n",
    "        context = sent[i-delta:i]\n",
    "        target = sent[i]\n",
    "        ngram = context + [target]\n",
    "        prob = prob_bigram_add_one(ngram, len(unigrams))\n",
    "        s += -log(prob,2)\n",
    "        N += 1\n",
    "        \n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(str(cross_entropy) + \" Cross Entropy\") #10.061359048452715 Cross Entropy\n",
    "print(str(perplexity) + \" Perplexity\") #1068.4910057009279 Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,2) #used to contruct MDF 2 Vocab from training set.\n",
    "bigrams, bigram_context = find_bigrams(fileTraining, unigrams,2) #trains a bigram\n",
    "\n",
    "#add one smoothing implementation\n",
    "\n",
    "testData = readIn(fileTesting, 1)\n",
    "\n",
    "s = 0\n",
    "N = 0\n",
    "delta = 1\n",
    "\n",
    "for sent in testData: # standard calculation of the cross entropy\n",
    "    for i in range(delta, len(sent)):\n",
    "        context = sent[i-delta:i]\n",
    "        target = sent[i]\n",
    "        ngram = context + [target]\n",
    "        prob = prob_bigram_add_k(ngram, len(unigrams), 100) #the prob of a bigram using add k.\n",
    "        s += -log(prob,2)\n",
    "        N += 1\n",
    "        \n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(str(cross_entropy) + \" Cross Entropy\")\n",
    "print(str(perplexity) + \" Perplexity\")\n",
    "\n",
    "'''\n",
    "the perplexity drops the lower the k value\n",
    "k = 0.2\n",
    "9.404859340160199 Cross Entropy\n",
    "677.8674282013725 Perplexity\n",
    "\n",
    "k = 0.4\n",
    "9.639059948911317 Cross Entropy\n",
    "797.3448153618554 Perplexity\n",
    "\n",
    "k = 0.6\n",
    "9.810956888083794 Cross Entropy\n",
    "898.2398314874424 Perplexity\n",
    "\n",
    "k = 0.8\n",
    "9.947594955533061 Cross Entropy\n",
    "987.4712641728472 Perplexity\n",
    "\n",
    "k = 2\n",
    "10.451833167515845 Cross Entropy\n",
    "1400.6037767581658 Perplexity\n",
    "\n",
    "k = 100 interested to see if the trend continues. (It does!)\n",
    "12.723824005557153 Cross Entropy\n",
    "6764.764546838359 Perplexity\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "# Kneser-Ney smoothing\n",
    "order = 4\n",
    "discount = 0.9\n",
    "\n",
    "unigram_denominator = 0\n",
    "ngram_numerator_map = Counter() \n",
    "ngram_denominator_map = Counter() \n",
    "ngram_non_zero_map = Counter()\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,2) #unigram keys = vocab\n",
    "corpus =  readInUnkOOVRejoin(fileTraining, unigrams)#create unkified training data.\n",
    "\n",
    "# train the model\n",
    "\n",
    "for line in corpus:\n",
    "    tokens = tokenize_sentence(line, order)\n",
    "    ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator =\\\n",
    "            ngrams_interpolated_kneser_ney(tokens,\n",
    "                                           order,\n",
    "                                           ngram_numerator_map,\n",
    "                                           ngram_denominator_map,\n",
    "                                           ngram_non_zero_map,\n",
    "                                           unigram_denominator)\n",
    "    \n",
    "heldoutCorpus = readInUnkOOVRejoin(fileHeldOut, unigrams) # read in and UNK OOV words in heldout corpus\n",
    "\n",
    "s = 0\n",
    "N = 0\n",
    "delta = order - 1 \n",
    "\n",
    "for sent in heldoutCorpus:\n",
    "    temp = sent.split()\n",
    "    #print(temp)\n",
    "    for i in range(delta, len(temp)):\n",
    "        context = temp[i-delta:i]\n",
    "        target = temp[i]\n",
    "        ngram = context + [target]\n",
    "        #print(ngram)\n",
    "        prob = kneser_ney_ngram_prob(ngram,discount,order);\n",
    "        s += -log(prob,2)\n",
    "        N += 1\n",
    "\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(str(cross_entropy) + \" Cross Entropy\")\n",
    "print(str(perplexity) + \" Perplexity\")\n",
    "\n",
    "testCorpus = readInUnkOOVRejoin(fileTesting, unigrams)\n",
    "\n",
    "s = 0\n",
    "N = 0\n",
    "delta = order - 1\n",
    "\n",
    "#probability calculation over the test corpus using kneser_ney\n",
    "for sent in testCorpus:\n",
    "    temp = sent.split()\n",
    "    #print(temp)\n",
    "    for i in range(delta, len(temp)):\n",
    "        context = temp[i-delta:i]\n",
    "        target = temp[i]\n",
    "        ngram = context + [target]\n",
    "        #print(ngram)\n",
    "        prob = kneser_ney_ngram_prob(ngram,discount,order); \n",
    "        s += -log(prob,2)\n",
    "        N += 1\n",
    "\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(str(cross_entropy) + \" Cross Entropy\")\n",
    "print(str(perplexity) + \" Perplexity\")\n",
    "\n",
    "'''\n",
    "Discount, Order\n",
    "0.2 3\n",
    "7.320822904064247 Cross Entropy\n",
    "159.87747735135233 Perplexity\n",
    "\n",
    "0.4 3\n",
    "6.893463689938024 Cross Entropy\n",
    "118.88836231370918 Perplexity\n",
    "\n",
    "0.6 3\n",
    "6.676621625215499 Cross Entropy\n",
    "102.29711336245768 Perplexity\n",
    "\n",
    "0.8 3\n",
    "6.568029012633623 Cross Entropy\n",
    "94.8797966432974 Perplexity\n",
    "\n",
    "0.2 4\n",
    "7.975827045678766 Cross Entropy\n",
    "251.74634908436195 Perplexity\n",
    "\n",
    "0.4 4\n",
    "7.246896062520934 Cross Entropy\n",
    "151.89136684868024 Perplexity\n",
    "\n",
    "0.6 4\n",
    "6.862635118751278 Cross Entropy\n",
    "116.37481940672485 Perplexity\n",
    "\n",
    "0.8 4\n",
    "6.648434145960116 Cross Entropy\n",
    "100.317823734305 Perplexity\n",
    "\n",
    "0.9 4                            The lowest perplexity\n",
    "6.603638530964016 Cross Entropy\n",
    "97.25082144272686 Perplexity\n",
    "\n",
    "1.0 4 \n",
    "6.708794374138775 Cross Entropy\n",
    "104.6040118802196 Perplexity\n",
    "0.2 5\n",
    "8.3208471723169 Cross Entropy\n",
    "319.7603334966111 Perplexity\n",
    "\n",
    "0.4 5\n",
    "7.454968793527132 Cross Entropy\n",
    "175.4564037552623 Perplexity\n",
    "\n",
    "0.6 5\n",
    "6.9923544869937855 Cross Entropy\n",
    "127.32346260634942 Perplexity\n",
    "\n",
    "0.8 5\n",
    "6.725611226436778 Cross Entropy\n",
    "105.8304684089911 Perplexity\n",
    "\n",
    "The cross entryopy is 6.64575074582797 and the Perplexity is 100.13140688195276.\n",
    "On the test data when the order is 4 and the discount is 0.9\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
