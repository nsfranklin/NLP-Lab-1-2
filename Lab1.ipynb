{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_tokens(tokens, order):\n",
    "    \"\"\"A useful way of glueing tokens together for\n",
    "    Kneser Ney smoothing and other smoothing methods\n",
    "    \n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    return '{0}@{1}'.format(order,' '.join(tokens))\n",
    "\n",
    "def unglue_tokens(tokenstring, order):\n",
    "    \"\"\"Ungluing tokens glued by the glue_tokens method\"\"\"\n",
    "    if order == 1:\n",
    "        return [tokenstring.split(\"@\")[1].replace(\" \",\"\")]\n",
    "    return tokenstring.split(\"@\")[1].split(\" \")\n",
    "\n",
    "def tokenize_sentence(sentence, order):\n",
    "    \"\"\"Returns a list of tokens with the correct numbers of initial\n",
    "    and end tags (this is meant ot be used with a non-backoff model!!!)\n",
    "    \n",
    "    :sentence: a string of text\n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    tokens = ['<s>'] * (order-1) + tokens + ['</s>']\n",
    "    return tokens\n",
    "\n",
    "def find_unigrams(filename, MDF): # ex 1\n",
    "    dataSource = open(filename, 'r')\n",
    "    unigrams = Counter()\n",
    "\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,1)\n",
    "        for w in words:\n",
    "            unigrams[w] += 1\n",
    "\n",
    "    unigramToRemove = [k for k in unigrams if unigrams[k] < MDF] #Combind training data pass 1 and 2 into one pass.\n",
    "    for w in unigramToRemove:\n",
    "        unigrams[\"<unk/>\"] += unigrams[w] #allow for variable MDF\n",
    "        del unigrams[w]\n",
    "\n",
    "    return unigrams\n",
    "\n",
    "def readInAndUnkOOV(filename, vocabCount): # exercise 1\n",
    "    dataSource = open(filename, 'r')\n",
    "    dataSansOOV = [];\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,1)\n",
    "        words[:] = ['<unk/>' if not(x in vocabCount) else x for x in words]\n",
    "        dataSansOOV.append(words)\n",
    "       \n",
    "    return dataSansOOV\n",
    "        \n",
    "def find_bigrams(filename, vocab, MDF): # exercise 2. Vocab represented by Unigram\n",
    "    dataSource = open(filename, 'r')\n",
    "    dataSansOOV = [];\n",
    "    for line in dataSource:\n",
    "        words = line.split(\" \")\n",
    "        words[:] = ['<unk/>' if not(x in vocab) else x for x in words]\n",
    "        dataSansOOV.append(' '.join(words)) #rejoins the words now Unkified        \n",
    "    \n",
    "\n",
    "    bigrams = Counter() # a counter for how many times a given bigram sequence w_i-1,w_i occurs\n",
    "    bigram_context = Counter() # a counter for how many times each word is used as a context word w_i-1 (so will include the start symbol)\n",
    "    delta = 1  # delta is order - 1\n",
    "    for line in dataSansOOV:\n",
    "        words = tokenize_sentence(line, 2)  # tokenize sentence with the order 2 as the parameter\n",
    "        for i in range(delta, len(words)):\n",
    "            context = words[i-delta:i]\n",
    "            target = words[i]\n",
    "            ngram = context + [target]\n",
    "            bigrams[glue_tokens(ngram, 2)] +=1\n",
    "            bigram_context[glue_tokens(context, 1)] += 1\n",
    "    print(len(bigrams.keys()), \"different bigrams\")\n",
    "    print(len(bigram_context.keys()), \"different bigram contexts (and unigrams) observed\")\n",
    "\n",
    "    return bigrams, bigram_context\n",
    "        \n",
    "def prob_bigram_add_one(ngram, i):\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = (numerator + 1) / (denominator + i)\n",
    "    return prob\n",
    "\n",
    "def prob_bigram_add_k(ngram, i, k):\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = (numerator + k) / (denominator + k*i)\n",
    "    return prob\n",
    "\n",
    "def prob_bigram_MLE(ngram):\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = numerator / denominator\n",
    "    return prob\n",
    "\n",
    "def readIn(filename, mode): # execise 2 mostly Reads in a file and splits it.\n",
    "    dataSource = open(filename, 'r')\n",
    "    data = [];\n",
    "    if mode == 1:\n",
    "        for line in dataSource:\n",
    "            words = tokenize_sentence(line,1)\n",
    "            data.append(words)\n",
    "    else:\n",
    "        count = 0\n",
    "        for line in dataSource:\n",
    "            words = tokenize_sentence(line,1)\n",
    "            data.append(words)\n",
    "            count += 1\n",
    "            if count > 1000:\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12730 Unique unigrams observed\n",
      "unigram total 1306313\n",
      "Test corpus cross entropy 8.287702715147693\n",
      "Test corpus perplexity 312.4979066155026\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,2) #the key values of the unigram are also a vocab.\n",
    "\n",
    "unigram_total = sum(unigrams.values());\n",
    "print(len(unigrams), \"Unique unigrams observed\")\n",
    "print(\"unigram total\", unigram_total)\n",
    "\n",
    "testData = readInAndUnkOOV(fileTesting, unigrams)\n",
    "\n",
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s \n",
    "for sent in testData:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for w in sent:\n",
    "        #print(str(unigrams[w]) + \" \" + w)\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        s += -log(prob, 2) # add the neg log prob to s\n",
    "        sent_s += -log(prob, 2)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N\n",
    "    sent_perplexity = 2 ** sent_cross_entropy\n",
    "    #print(sent, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(\"Test corpus cross entropy\", cross_entropy)\n",
    "print(\"Test corpus perplexity\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194181 different bigrams\n",
      "18076 different bigram contexts (and unigrams) observed\n",
      "10.585434210610824 Cross Entropy\n",
      "1536.5022994003118 Perplexity\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,1) #used to contruct MDF 2 Vocab from training set.\n",
    "bigrams, bigram_context = find_bigrams(fileTraining, unigrams,2) #trains a bigram\n",
    "\n",
    "#add one smoothing implementation\n",
    "\n",
    "testData = readIn(fileTesting, 1)\n",
    "\n",
    "s = 0\n",
    "N = 0\n",
    "delta = 1\n",
    "\n",
    "for sent in testData:\n",
    "    for i in range(delta, len(sent)):\n",
    "        context = sent[i-delta:i]\n",
    "        target = sent[i]\n",
    "        ngram = context + [target]\n",
    "        prob = prob_bigram_add_one(ngram, len(unigrams))\n",
    "        s += -log(prob,2)\n",
    "        N += 1\n",
    "        \n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(str(cross_entropy) + \" Cross Entropy\") #10.585434210610824 Cross Entropy\n",
    "print(str(perplexity) + \" Perplexity\") #1536.5022994003118 Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194181 different bigrams\n",
      "18076 different bigram contexts (and unigrams) observed\n",
      "13.416189879965472 Cross Entropy\n",
      "10931.394844263728 Perplexity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nthe perplexity drops the lower the k value\\nk = 0.2\\n9.780690090093266 Cross Entropy\\n879.5916448935646 Perplexity\\n\\nk = 0.4\\n10.07927867287786 Cross Entropy\\n1081.8454308700907 Perplexity\\n\\nk = 0.6\\n10.289039119808113 Cross Entropy\\n1251.1500615693983 Perplexity\\n\\nk = 0.8\\n10.451935305220518 Cross Entropy\\n1400.7029380603492 Perplexity\\n\\nk = 2\\n11.031646677383636 Cross Entropy\\n2093.4208795525756 Perplexity\\n\\nk = 100 interested to see if the trend continues. (It does!)\\n\\n\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,1) #used to contruct MDF 2 Vocab from training set.\n",
    "bigrams, bigram_context = find_bigrams(fileTraining, unigrams,2) #trains a bigram\n",
    "\n",
    "#add one smoothing implementation\n",
    "\n",
    "testData = readIn(fileTesting, 1)\n",
    "\n",
    "s = 0\n",
    "N = 0\n",
    "delta = 1\n",
    "\n",
    "for sent in testData:\n",
    "    for i in range(delta, len(sent)):\n",
    "        context = sent[i-delta:i]\n",
    "        target = sent[i]\n",
    "        ngram = context + [target]\n",
    "        prob = prob_bigram_add_k(ngram, len(unigrams), 100)\n",
    "        s += -log(prob,2)\n",
    "        N += 1\n",
    "        \n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(str(cross_entropy) + \" Cross Entropy\")\n",
    "print(str(perplexity) + \" Perplexity\")\n",
    "\n",
    "'''\n",
    "the perplexity drops the lower the k value\n",
    "k = 0.2\n",
    "9.780690090093266 Cross Entropy\n",
    "879.5916448935646 Perplexity\n",
    "\n",
    "k = 0.4\n",
    "10.07927867287786 Cross Entropy\n",
    "1081.8454308700907 Perplexity\n",
    "\n",
    "k = 0.6\n",
    "10.289039119808113 Cross Entropy\n",
    "1251.1500615693983 Perplexity\n",
    "\n",
    "k = 0.8\n",
    "10.451935305220518 Cross Entropy\n",
    "1400.7029380603492 Perplexity\n",
    "\n",
    "k = 2\n",
    "11.031646677383636 Cross Entropy\n",
    "2093.4208795525756 Perplexity\n",
    "\n",
    "k = 100 interested to see if the trend continues. (It does!)\n",
    "13.416189879965472 Cross Entropy\n",
    "10931.394844263728 Perplexity\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
