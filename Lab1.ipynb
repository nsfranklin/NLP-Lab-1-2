{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_tokens(tokens, order):\n",
    "    \"\"\"A useful way of glueing tokens together for\n",
    "    Kneser Ney smoothing and other smoothing methods\n",
    "    \n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    return '{0}@{1}'.format(order,' '.join(tokens))\n",
    "\n",
    "def unglue_tokens(tokenstring, order):\n",
    "    \"\"\"Ungluing tokens glued by the glue_tokens method\"\"\"\n",
    "    if order == 1:\n",
    "        return [tokenstring.split(\"@\")[1].replace(\" \",\"\")]\n",
    "    return tokenstring.split(\"@\")[1].split(\" \")\n",
    "\n",
    "def tokenize_sentence(sentence, order):\n",
    "    \"\"\"Returns a list of tokens with the correct numbers of initial\n",
    "    and end tags (this is meant ot be used with a non-backoff model!!!)\n",
    "    \n",
    "    :sentence: a string of text\n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    tokens = ['<s>'] * (order-1) + tokens + ['</s>']\n",
    "    return tokens\n",
    "\n",
    "def find_unigrams(file, MDF): # ex 1\n",
    "    dataSource = open(filename, 'r')\n",
    "    unigrams = Counter()\n",
    "\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,1)\n",
    "        for w in words:\n",
    "            unigrams[w] += 1\n",
    "\n",
    "    unigramToRemove = [k for k in unigrams if unigrams[k] < MDF] #Combind training data pass 1 and 2 into one pass.\n",
    "    for w in unigramToRemove:\n",
    "        unigrams[\"<unk/>\"] += unigrams[w] \n",
    "        del unigrams[w]\n",
    "\n",
    "    return unigrams\n",
    "\n",
    "def readInAndUnkOOV(filename, vocabCount): # exercise 1\n",
    "    dataSource = open(filename, 'r')\n",
    "    dataSansOOV = [];\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,1)\n",
    "        words[:] = ['<unk/>' if not(x in vocabCount) else x for x in words]\n",
    "        dataSansOOV.append(words)\n",
    "                \n",
    "    return dataSansOOV\n",
    "        \n",
    "def find_bigrams(file, MDF): # ex 1\n",
    "    dataSource = open(filename, 'r')\n",
    "    unigrams = Counter()\n",
    "\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,2)\n",
    "        for w in words:\n",
    "            unigrams[w] += 1\n",
    "\n",
    "    bigramToRemove = [k for k in unigrams if unigrams[k] < MDF] #Combind training data pass 1 and 2 into one pass.\n",
    "    for w in unigramToRemove:\n",
    "        unigrams[\"<unk/>\"] += unigrams[w] \n",
    "        del unigrams[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12730 Unique unigrams observed\n",
      "unigram total 1306313\n",
      "Test corpus cross entropy 8.287702715147693\n",
      "Test corpus perplexity 312.4979066155026\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,2)\n",
    "\n",
    "unigram_total = sum(unigrams.values());\n",
    "print(len(unigrams), \"Unique unigrams observed\")\n",
    "print(\"unigram total\", unigram_total)\n",
    "\n",
    "trainingData = readInAndUnkOOV(fileTesting, unigrams)\n",
    "\n",
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s \n",
    "for sent in trainingData:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for w in sent:\n",
    "        #print(str(unigrams[w]) + \" \" + w)\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        logprob = log(prob, 2)  # the log of the prob to base 2\n",
    "        s += -log(prob, 2) # add the neg log prob to s\n",
    "        sent_s += -log(prob, 2)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N\n",
    "    sent_perplexity = 2 ** sent_cross_entropy\n",
    "    #print(sent, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(\"Test corpus cross entropy\", cross_entropy)\n",
    "print(\"Test corpus perplexity\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "bigrams = find_bigrams(fileTraining,2)\n",
    "\n",
    "unigram_total = sum(unigrams.values());\n",
    "print(len(unigrams), \"Unique unigrams observed\")\n",
    "print(\"unigram total\", unigram_total)\n",
    "\n",
    "trainingData = readInAndUnkOOV(fileTesting, unigrams)\n",
    "\n",
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s \n",
    "for sent in trainingData:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for w in sent:\n",
    "        #print(str(unigrams[w]) + \" \" + w)\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        logprob = log(prob, 2)  # the log of the prob to base 2\n",
    "        s += -log(prob, 2) # add the neg log prob to s\n",
    "        sent_s += -log(prob, 2)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N\n",
    "    sent_perplexity = 2 ** sent_cross_entropy\n",
    "    #print(sent, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(\"Test corpus cross entropy\", cross_entropy)\n",
    "print(\"Test corpus perplexity\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
