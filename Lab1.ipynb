{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_tokens(tokens, order):\n",
    "    \"\"\"A useful way of glueing tokens together for\n",
    "    Kneser Ney smoothing and other smoothing methods\n",
    "    \n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    return '{0}@{1}'.format(order,' '.join(tokens))\n",
    "\n",
    "def unglue_tokens(tokenstring, order):\n",
    "    \"\"\"Ungluing tokens glued by the glue_tokens method\"\"\"\n",
    "    if order == 1:\n",
    "        return [tokenstring.split(\"@\")[1].replace(\" \",\"\")]\n",
    "    return tokenstring.split(\"@\")[1].split(\" \")\n",
    "\n",
    "def tokenize_sentence(sentence, order):\n",
    "    \"\"\"Returns a list of tokens with the correct numbers of initial\n",
    "    and end tags (this is meant ot be used with a non-backoff model!!!)\n",
    "    \n",
    "    :sentence: a string of text\n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    tokens = ['<s>'] * (order-1) + tokens + ['</s>']\n",
    "    return tokens\n",
    "\n",
    "def find_unigrams(filename, MDF): # ex 1\n",
    "    dataSource = open(filename, 'r')\n",
    "    unigrams = Counter()\n",
    "\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,1)\n",
    "        for w in words:\n",
    "            unigrams[w] += 1\n",
    "\n",
    "    unigramToRemove = [k for k in unigrams if unigrams[k] < MDF] #Combind training data pass 1 and 2 into one pass.\n",
    "    for w in unigramToRemove:\n",
    "        unigrams[\"<unk/>\"] += unigrams[w] #allow for variable MDF\n",
    "        del unigrams[w]\n",
    "\n",
    "    return unigrams\n",
    "\n",
    "def readInAndUnkOOV(filename, vocabCount): # exercise 1\n",
    "    dataSource = open(filename, 'r')\n",
    "    dataSansOOV = [];\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,1)\n",
    "        words[:] = ['<unk/>' if not(x in vocabCount) else x for x in words]\n",
    "        dataSansOOV.append(words)\n",
    "       \n",
    "    return dataSansOOV\n",
    "        \n",
    "def find_bigrams(filename, vocab, MDF): # exercise 2. Vocab represented by Unigram\n",
    "    dataSource = open(filename, 'r')\n",
    "    dataSansOOV = [];\n",
    "    for line in dataSource:\n",
    "        words = line.split(\" \")\n",
    "        words[:] = ['<unk/>' if not(x in vocab) else x for x in words]\n",
    "        dataSansOOV.append(' '.join(words)) #rejoins the words now Unkified        \n",
    "    \n",
    "\n",
    "    bigrams = Counter() # a counter for how many times a given bigram sequence w_i-1,w_i occurs\n",
    "    bigram_context = Counter() # a counter for how many times each word is used as a context word w_i-1 (so will include the start symbol)\n",
    "    delta = 1  # delta is order - 1\n",
    "    for line in dataSansOOV:\n",
    "        words = tokenize_sentence(line, 2)  # tokenize sentence with the order 2 as the parameter\n",
    "        #print(words)\n",
    "        for i in range(delta, len(words)):\n",
    "            context = words[i-delta:i]\n",
    "            target = words[i]\n",
    "            ngram = context + [target]\n",
    "            bigrams[glue_tokens(ngram, 2)] +=1\n",
    "            bigram_context[glue_tokens(context, 1)] += 1\n",
    "    print(len(bigrams.keys()), \"different bigrams\")\n",
    "    print(len(bigram_context.keys()), \"different bigram contexts (and unigrams) observed\")\n",
    "\n",
    "    return bigrams, bigram_context\n",
    "        \n",
    "def prob_bigram_MLE(ngram):\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = numerator / denominator\n",
    "    return prob\n",
    "\n",
    "def readIn(filename): # execise 2 mostly Reads in a file and splits it.\n",
    "    dataSource = open(filename, 'r')\n",
    "    data = [];\n",
    "    for line in dataSource:\n",
    "        words = tokenize_sentence(line,1)\n",
    "        data.append(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,2) #the key values of the unigram are also a vocab.\n",
    "\n",
    "unigram_total = sum(unigrams.values());\n",
    "print(len(unigrams), \"Unique unigrams observed\")\n",
    "print(\"unigram total\", unigram_total)\n",
    "\n",
    "testData = readInAndUnkOOV(fileTesting, unigrams)\n",
    "\n",
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s \n",
    "for sent in testData:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for w in sent:\n",
    "        #print(str(unigrams[w]) + \" \" + w)\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        logprob = log(prob, 2)  # the log of the prob to base 2\n",
    "        s += -log(prob, 2) # add the neg log prob to s\n",
    "        sent_s += -log(prob, 2)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N\n",
    "    sent_perplexity = 2 ** sent_cross_entropy\n",
    "    #print(sent, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N\n",
    "perplexity = 2 ** cross_entropy\n",
    "print(\"Test corpus cross entropy\", cross_entropy)\n",
    "print(\"Test corpus perplexity\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194181 different bigrams\n",
      "18076 different bigram contexts (and unigrams) observed\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "fileTraining = 'switchboard_lm_train.txt'\n",
    "fileTesting = 'switchboard_lm_test.txt'\n",
    "fileHeldOut = 'switchboard_lm_heldout.txt'\n",
    "\n",
    "unigrams = find_unigrams(fileTraining,1) #used to contruct MDF 2 Vocab from training set.\n",
    "bigrams, bigram_context = find_bigrams(fileTraining, unigrams,2) #trains a bigram\n",
    "\n",
    "''' \n",
    "for x in bigrams: \n",
    "    print(x) # for checking the bigram functions!\n",
    "    print(bigrams[x])\n",
    "''' \n",
    "\n",
    "#add one smoothing implementation\n",
    "\n",
    "testData = readIn(fileTesting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
